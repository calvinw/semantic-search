{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Small Similarity Example\n",
        "\n",
        "## Similarities Between Sentences\n",
        "\n",
        "We will look at the similarities between 3 sentences:\n",
        "\n",
        "-   “I like to be in my house”,\n",
        "-   “I enjoy staying home”,\n",
        "-   “The isotope 238u decays to 206pb”\n",
        "\n",
        "Clearly the first two sentences are very similar, and the last one is\n",
        "very different from the first two. We should see that reflected in the\n",
        "similarities that are computed from the embeddings.\n",
        "\n",
        "First we compute the similarities between them"
      ],
      "id": "0fef86e1-a1f2-47e9-add4-04a8c157ecab"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import textwrap\n",
        "\n",
        "# Set NumPy print options to display numbers in decimal notation\n",
        "np.set_printoptions(suppress=True, precision=6)\n",
        "\n",
        "# Define our texts\n",
        "texts = [\"I like to be in my house\", \n",
        "         \"I enjoy staying home\", \n",
        "         \"The isotope 238u decays to 206pb\"]\n",
        "\n",
        "# Check if GPU is available and use it if possible\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# Load the model\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2', device=device)\n",
        "\n",
        "# Generate embeddings\n",
        "embeddings = model.encode(texts)\n",
        "\n",
        "# Compute pairwise similarities\n",
        "similarities = cosine_similarity(embeddings)"
      ],
      "id": "9fe96d27"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### How many dimensions in our embedding?"
      ],
      "id": "d60bd653-9cb7-4843-aaa6-208fba29823e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Number of dimensions for sentence 1\")\n",
        "print(len(embeddings[0]))\n",
        "print(\"Number of dimensions for sentence 2\")\n",
        "print(len(embeddings[1]))\n",
        "print(\"Number of dimensions for sentence 3\")\n",
        "print(len(embeddings[2]))"
      ],
      "id": "8a521d96"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### The embedding for ‘I like to be in my house’"
      ],
      "id": "0e95b14a-2b3d-41ca-9423-3ef24ca54240"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"This is the embedding for 'I like to be in my house' (first 10 out of 384 entries)\")\n",
        "print(embeddings[0][:10])"
      ],
      "id": "d54a4e98"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### The embedding for ‘I enjoy staying home’"
      ],
      "id": "443f1d40-eee1-4647-936c-4e2baea07c61"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"This is the embedding for 'I enjoy staying home' (first 10 out of 384 entries)\")\n",
        "print(embeddings[1][:10])"
      ],
      "id": "e5e044b7"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### The embedding for ‘The isotope 238 decays to 206pb’"
      ],
      "id": "d23a4f55-43ff-40e3-baeb-460a3d8ed609"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"This is the embedding for 'The isotope 238 decays to 206pb' (first 10 out of 384 entries)\")\n",
        "print(embeddings[2][:10])"
      ],
      "id": "9d2fd33d"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Now we create a heatmap that shows the similarities"
      ],
      "id": "86c9fe89-3120-41ec-9194-12eac03563ee"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a heatmap\n",
        "plt.figure(figsize=(8, 8))\n",
        "ax = sns.heatmap(similarities, annot=True, cmap='Blues', vmin=0, vmax=1, square=True, fmt='.4f')\n",
        "\n",
        "# Remove x-axis ticks\n",
        "ax.set_xticks([])\n",
        "\n",
        "# Set y-axis tick labels with wrapping\n",
        "wrapped_texts = ['\\n'.join(textwrap.wrap(text, width=20)) for text in texts]\n",
        "ax.set_yticks(np.arange(len(texts)) + 0.5)\n",
        "ax.set_yticklabels(wrapped_texts, rotation=0, ha='right')\n",
        "\n",
        "# Add wrapped x-axis labels at the bottom\n",
        "ax.set_xlabel('')\n",
        "plt.xticks([])\n",
        "for i, text in enumerate(texts):\n",
        "    wrapped_text = '\\n'.join(textwrap.wrap(text, width=20))\n",
        "    plt.text(i + 0.5, len(texts) + 0.1, wrapped_text,\n",
        "             horizontalalignment='center',\n",
        "             verticalalignment='bottom',\n",
        "             rotation=0,\n",
        "             fontsize=10)\n",
        "\n",
        "# Adjust the bottom margin to accommodate wrapped labels\n",
        "plt.title('Sentence Similarities')\n",
        "plt.tight_layout()\n",
        "plt.subplots_adjust(bottom=0.2)  # Increase bottom margin\n",
        "plt.show()"
      ],
      "id": "446ac0b9"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python"
    }
  }
}