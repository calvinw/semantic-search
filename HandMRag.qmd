---
title: "Rag for H and M"
format:
  html:
    page-layout: full
---

{{< include _llm_openrouter.qmd >}}

```{python}
#| eval: false 
#| echo: false 
#| output: false
from dotenv import load_dotenv
load_dotenv()
```

```{python}
import numpy as np
import pandas as pd 
from sentence_transformers import SentenceTransformer
import time
from io import StringIO
from io import BytesIO
from typing import List, Tuple
import requests
```

```{python}
class ProductSearcher:
    def __init__(self, embeddings_url: str = "https://github.com/calvinw/semantic-search/raw/refs/heads/main/product_embeddings.npz"):
        """Initialize the searcher with embeddings from GitHub URL."""
        print("Downloading embeddings file from GitHub...")
        try:
            response = requests.get(embeddings_url)
            response.raise_for_status()
            
            # Load the embeddings from the downloaded content
            print("Loading embeddings and model...")
            self.data = np.load(BytesIO(response.content), allow_pickle=True)
            self.embeddings = self.data['embeddings']
            self.product_names = self.data['product_names']
            self.embedding_strings = self.data['embedding_strings']
            self.product_codes = self.data['product_codes']
            self.article_ids_str = self.data['article_ids_str']
            
            # Load the model
            print("Loading sentence transformer model...")
            self.model = SentenceTransformer('all-MiniLM-L6-v2')
            print("Model loaded successfully!")
            
            print(f"Loaded {len(self.embeddings)} products")
            
        except Exception as e:
            print(f"Error in initialization: {e}")
            raise

    def get_product_name(self, idx):
        return self.product_names[idx]

    def get_embedding_string(self, idx):
        return self.embedding_strings[idx]
    
    def search(self, query: str, top_k: int = 4) -> List[Tuple[int, float]]:
        """Search for products using a text query."""
        query_embedding = self.model.encode([query])[0]
        
        similarities = np.dot(self.embeddings, query_embedding) / (
            np.linalg.norm(self.embeddings, axis=1) * np.linalg.norm(query_embedding)
        )
        
        top_idx = np.argsort(similarities)[::-1][:top_k]
        return [(idx, similarities[idx]) for idx in top_idx]
    
searcher = ProductSearcher()
```

## A Search and Results

```{python}
query="Socks for holiday"
hits=searcher.search(query, 3)

idx_first = hits[0][0]
idx_second = hits[1][0]
idx_third = hits[2][0]

score1 = hits[0][1]
score2 = hits[1][1]
score3 = hits[2][1]

match1 = searcher.get_embedding_string(idx_first)
match2 = searcher.get_embedding_string(idx_second)
match3 = searcher.get_embedding_string(idx_third)

match1 = str(score1) + "|" + match1
match2 = str(score2) + "|" + match2
match3 = str(score3) + "|" + match3

print(match1)
```

## Bundle the Matches and Send to LLM 

Let's set up a prompt to ask the LLM to discuss the hits and ask them if they
want more information about the items.   

```{python}

template="""
Below are three matched items that were returned by vector search of a user
query, which is in the <query> </query> tags. The first number is the
similarity score of the match. 

Please formulate a response to the user's query that describes the 3 items and
asks the user if they 

<query>
{query}
</query>

<match>
{match1}
</match>

<match>
{match2}
</match>

<match>
{match3}
</match>
"""

prompt=template.format(match1=match1, match2=match2, match3=match3, query=query)
response = get_completion(prompt)

print("Prompt:")
print(prompt)
print("Response:")
print(response)
```
