{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Rag for H and M\n",
        "\n",
        "### Put Your OPENROUTER_API_KEY here"
      ],
      "id": "9f555baf-9cbb-4502-8a76-9466667a0362"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"OPENROUTER_API_KEY\"] = \"paste_your_api_key_here\""
      ],
      "id": "54dc5b1b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip3 install langchain langchain_openai"
      ],
      "id": "2f693048"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "try:\n",
        "    model_name\n",
        "except NameError:\n",
        "    model_name=\"openai/gpt-4o-mini\"\n",
        "\n",
        "print(\"Model Name:\", model_name)\n",
        "print(\"Provider:\", \"OpenRouter AI\")\n",
        "\n",
        "llm=ChatOpenAI(model_name=model_name,\n",
        "               openai_api_key=os.environ.get(\"OPENROUTER_API_KEY\"),\n",
        "               openai_api_base=\"https://openrouter.ai/api/v1\")"
      ],
      "id": "6a5e9a1f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import textwrap\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "from IPython.display import display, clear_output, Markdown\n",
        "from ipywidgets import widgets, Layout\n",
        "\n",
        "conversation_output = widgets.Output()\n",
        "messages = []\n",
        "\n",
        "def run_chatbot(system_prompt, initial_message):\n",
        "    global messages \n",
        "    messages = [ {'role':'system', 'content': system_prompt} ]\n",
        "    conversation_output.clear_output()\n",
        "\n",
        "    messages.append({'role': 'assistant', 'content': initial_message})\n",
        "\n",
        "    text_input = widgets.Text(\n",
        "        placeholder='Type your message here...',\n",
        "        layout=widgets.Layout(width='50%')\n",
        "    )\n",
        "    submit_button = widgets.Button(description=\"Send\")\n",
        "\n",
        "    input_box = widgets.HBox([text_input, submit_button])\n",
        "    display(conversation_output, input_box)\n",
        "\n",
        "    def on_submit_click(b):\n",
        "        message = text_input.value\n",
        "        text_input.value = ''  # Clear the input field\n",
        "\n",
        "        with conversation_output:\n",
        "            display(Markdown(f\"**User**: {message}\"))\n",
        "            messages.append({'role': 'user', 'content': message})\n",
        "            response = get_completion_messages(messages)\n",
        "            display(Markdown(f\"**AI**: {response}\"))\n",
        "            messages.append({'role': 'assistant', 'content': response})\n",
        "\n",
        "    submit_button.on_click(on_submit_click)\n",
        "\n",
        "    # Display initial AI message\n",
        "    with conversation_output:\n",
        "        display(Markdown(f\"**AI**: {initial_message}\"))\n",
        "\n",
        "def wrap_text(text, max_width=80):\n",
        "    \"\"\"\n",
        "    Wraps the text to the specified max_width, preserving line breaks and formatting.\n",
        "    \"\"\"\n",
        "    text = text.lstrip()\n",
        "    lines = text.splitlines()  # Split the text into lines\n",
        "    wrapped_lines = []\n",
        "    for line in lines:\n",
        "        if line.strip():  # Skip empty lines\n",
        "            wrapped_line = textwrap.fill(line, max_width, initial_indent='', subsequent_indent='')\n",
        "            wrapped_lines.extend(wrapped_line.splitlines())  # Preserve line breaks\n",
        "        else:\n",
        "            wrapped_lines.append('')  # Keep empty lines\n",
        "    return '\\n'.join(wrapped_lines)\n",
        "\n",
        "def print_messages():\n",
        "    for message in messages:\n",
        "        role = message['role']\n",
        "        content = message['content']\n",
        "        \n",
        "        if role == 'system':\n",
        "            print(\"System:\")\n",
        "            print(\"-\" * 40)\n",
        "            print(content)\n",
        "        elif role == 'user':\n",
        "            print(\"User: \", end=\"\")\n",
        "            print(wrap_text(content))\n",
        "        elif role == 'assistant':\n",
        "            print(\"Assistant: \", end=\"\")\n",
        "            print(wrap_text(content))\n",
        "        print()  # Add an extra newline for spacing\n",
        "\n",
        "def print_prompt_and_response(prompt, response):\n",
        "    print(\"Prompt: \")\n",
        "    print(wrap_text(prompt))\n",
        "    print(\"\")\n",
        "    print(\"Response: \")\n",
        "    print(response)\n",
        "\n",
        "def print_messages_and_response(messages, response):\n",
        "    prompt = ChatPromptTemplate(messages=messages)\n",
        "    print_prompt_and_response(prompt.format(), response)\n",
        "\n",
        "def get_completion(prompt, temperature=0.0):\n",
        "    response = llm.invoke(prompt, temperature=temperature)\n",
        "    wrapped_response = wrap_text(response.content)\n",
        "    return wrapped_response\n",
        "\n",
        "def get_completion_messages(messages, temperature=0.0):\n",
        "    response=llm.invoke(messages, temperature=temperature)\n",
        "    wrapped_response = wrap_text(response.content)\n",
        "    return wrapped_response"
      ],
      "id": "3b6126d3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd \n",
        "from sentence_transformers import SentenceTransformer\n",
        "import time\n",
        "from io import StringIO\n",
        "from io import BytesIO\n",
        "from typing import List, Tuple\n",
        "import requests"
      ],
      "id": "c1437204"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ProductSearcher:\n",
        "    def __init__(self, embeddings_url: str = \"https://github.com/calvinw/semantic-search/raw/refs/heads/main/product_embeddings.npz\"):\n",
        "        \"\"\"Initialize the searcher with embeddings from GitHub URL.\"\"\"\n",
        "        print(\"Downloading embeddings file from GitHub...\")\n",
        "        try:\n",
        "            response = requests.get(embeddings_url)\n",
        "            response.raise_for_status()\n",
        "            \n",
        "            # Load the embeddings from the downloaded content\n",
        "            print(\"Loading embeddings and model...\")\n",
        "            self.data = np.load(BytesIO(response.content), allow_pickle=True)\n",
        "            self.embeddings = self.data['embeddings']\n",
        "            self.product_names = self.data['product_names']\n",
        "            self.embedding_strings = self.data['embedding_strings']\n",
        "            self.product_codes = self.data['product_codes']\n",
        "            self.article_ids_str = self.data['article_ids_str']\n",
        "            \n",
        "            # Load the model\n",
        "            print(\"Loading sentence transformer model...\")\n",
        "            self.model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "            print(\"Model loaded successfully!\")\n",
        "            \n",
        "            print(f\"Loaded {len(self.embeddings)} products\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"Error in initialization: {e}\")\n",
        "            raise\n",
        "\n",
        "    def get_product_name(self, idx):\n",
        "        return self.product_names[idx]\n",
        "\n",
        "    def get_embedding_string(self, idx):\n",
        "        return self.embedding_strings[idx]\n",
        "    \n",
        "    def search(self, query: str, top_k: int = 4) -> List[Tuple[int, float]]:\n",
        "        \"\"\"Search for products using a text query.\"\"\"\n",
        "        query_embedding = self.model.encode([query])[0]\n",
        "        \n",
        "        similarities = np.dot(self.embeddings, query_embedding) / (\n",
        "            np.linalg.norm(self.embeddings, axis=1) * np.linalg.norm(query_embedding)\n",
        "        )\n",
        "        \n",
        "        top_idx = np.argsort(similarities)[::-1][:top_k]\n",
        "        return [(idx, similarities[idx]) for idx in top_idx]\n",
        "    \n",
        "searcher = ProductSearcher()"
      ],
      "id": "190b96ac"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## A Search and Results"
      ],
      "id": "01d05f2a-4748-42c6-a87e-bf95fad2d89d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "query=\"Socks for holiday\"\n",
        "hits=searcher.search(query, 3)\n",
        "\n",
        "idx_first = hits[0][0]\n",
        "idx_second = hits[1][0]\n",
        "idx_third = hits[2][0]\n",
        "\n",
        "score1 = hits[0][1]\n",
        "score2 = hits[1][1]\n",
        "score3 = hits[2][1]\n",
        "\n",
        "match1 = searcher.get_embedding_string(idx_first)\n",
        "match2 = searcher.get_embedding_string(idx_second)\n",
        "match3 = searcher.get_embedding_string(idx_third)\n",
        "\n",
        "match1 = str(score1) + \"|\" + match1\n",
        "match2 = str(score2) + \"|\" + match2\n",
        "match3 = str(score3) + \"|\" + match3\n",
        "\n",
        "print(match1)"
      ],
      "id": "364bf1a7"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bundle the Matches and Send to LLM\n",
        "\n",
        "Let’s set up a prompt to ask the LLM to discuss the hits and ask them if\n",
        "they want more information about the items."
      ],
      "id": "b6a58713-9b86-46e7-b1e3-2c75f0a311d3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "template=\"\"\"\n",
        "Below are three matched items that were returned by vector search of a user\n",
        "query, which is in the <query> </query> tags. The first number is the\n",
        "similarity score of the match. \n",
        "\n",
        "Please formulate a response to the user's query that describes the 3 items and\n",
        "asks the user if they \n",
        "\n",
        "<query>\n",
        "{query}\n",
        "</query>\n",
        "\n",
        "<match>\n",
        "{match1}\n",
        "</match>\n",
        "\n",
        "<match>\n",
        "{match2}\n",
        "</match>\n",
        "\n",
        "<match>\n",
        "{match3}\n",
        "</match>\n",
        "\"\"\"\n",
        "\n",
        "prompt=template.format(match1=match1, match2=match2, match3=match3, query=query)\n",
        "response = get_completion(prompt)\n",
        "\n",
        "print(\"Prompt:\")\n",
        "print(prompt)\n",
        "print(\"Response:\")\n",
        "print(response)"
      ],
      "id": "d1d20ac5"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "path": "/home/calvinw/develop/semantic-search/semantic-search-env/share/jupyter/kernels/python3"
    }
  }
}